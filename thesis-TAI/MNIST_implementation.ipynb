{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation on the MNIST DATASET\n",
        "\n",
        "**Robustness and Privacy**"
      ],
      "metadata": {
        "id": "KNNWjq3mYg4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and endpoint creation with Google Vertex AI"
      ],
      "metadata": {
        "id": "cbsjlSsYYlml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise Google Drive"
      ],
      "metadata": {
        "id": "alEx_1xYY16E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9sPcstpSbY0",
        "outputId": "cfba6a3b-e249-46db-eaa8-4a7626cad557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw28EN-iHG1I"
      },
      "source": [
        "Install the latest version of Vertex SDK and Google Cloud Storage SDK and then restart kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmAM27iXHQva"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "    \n",
        "\n",
        "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform\n",
        "! pip install {USER_FLAG} --upgrade google-cloud-storage\n",
        "! pip install {USER_FLAG} --upgrade numpy\n",
        "! pip install git+https://github.com/RobustBench/robustbench.git\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all libraries"
      ],
      "metadata": {
        "id": "Skhu04VYY4pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import gzip\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import torch\n",
        "from google.cloud import storage\n",
        "from keras.preprocessing.image import array_to_img\n",
        "from typing import Dict, Optional, Sequence, Tuple, List"
      ],
      "metadata": {
        "id": "UwSIPV-vXfzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDoYRk1GU_9V",
        "outputId": "6c731d12-8c03-4aaf-9674-bf9526462f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PXtug_GH23o"
      },
      "source": [
        "Set global variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48zj0wxKH4de"
      },
      "outputs": [],
      "source": [
        "PROJECT = \"mnist-automl-350516\"  # @param {type:\"string\"}\n",
        "REGION = \"europe-west4\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If not trained, set a new timestamp"
      ],
      "metadata": {
        "id": "yf1WgWhonWoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "print(TIMESTAMP)"
      ],
      "metadata": {
        "id": "fQZ-Li1mnTVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65387e10-71f3-4447-ff8b-75a2f18b5aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20220531151405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If already trained, get last timestamp"
      ],
      "metadata": {
        "id": "0YQbByvYm8ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TIMESTAMP = \"20220530154112\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "kJdWM93Km4dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXlL_i23IaXf"
      },
      "source": [
        "Authenticate to Google Cloud (if running on Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoy9jZjaIcqX"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5eR11tIz1f"
      },
      "source": [
        "Create a bucket to execute our code\n",
        "\n",
        "**This step is necessary**\n",
        "\n",
        "Set bucket name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://bucket-mnist-1\"  # @param {type:\"string\"}\n",
        "NO_PATH_BUCKET_NAME = \"bucket-mnist-1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz8J0vmSlugt"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def create_bucket_class_location():\n",
        "    \"\"\"\n",
        "    Create a new bucket in the settled region with the coldline storage\n",
        "    class\n",
        "    \"\"\"\n",
        "    bucket_name = NO_PATH_BUCKET_NAME\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    bucket.storage_class = \"STANDARD\"\n",
        "    new_bucket = storage_client.create_bucket(bucket, project=PROJECT, location=REGION)\n",
        "\n",
        "    print(\n",
        "        \"Created bucket {} in {} with storage class {}\".format(\n",
        "            new_bucket.name, new_bucket.location, new_bucket.storage_class\n",
        "        )\n",
        "    )\n",
        "    return new_bucket\n",
        "\n",
        "create_bucket_class_location()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uClULuven2Sg"
      },
      "source": [
        "Initialise AI Platform for our project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flCtqdvKn5ZO"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(\n",
        "    project=PROJECT,\n",
        "    location=REGION,\n",
        "    staging_bucket=BUCKET_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the function to load datset from a directory"
      ],
      "metadata": {
        "id": "IdZW-m_cY8Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist(path, kind='train'):\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784) \n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "c_A7tsKsbMRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset from Google Drive"
      ],
      "metadata": {
        "id": "ljkzQgKhZFif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import training data\n",
        "X_train, y_train = load_mnist(path='/content/drive/MyDrive/Colab Notebooks/mnist', kind='train')\n",
        "X_test, y_test = load_mnist(path='/content/drive/MyDrive/Colab Notebooks/mnist', kind='t10k')"
      ],
      "metadata": {
        "id": "YHDnX1VGYB7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dictionary to map all the images"
      ],
      "metadata": {
        "id": "PInHyBgt2xAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset placeholder\n",
        "files = pd.DataFrame({'part': np.concatenate([\n",
        "                                   np.repeat('TRAIN', 60000),\n",
        "                                   np.repeat('TEST', 10000)\n",
        "                              ]),\n",
        "                      'file': np.repeat('file', 70000),\n",
        "                      'label': np.repeat('label', 70000)})\n",
        "\n",
        "# Stack training and test data into single arrays\n",
        "X_data = np.vstack([X_train, X_test])\n",
        "y_data = np.concatenate([y_train, y_test])"
      ],
      "metadata": {
        "id": "iLPlX2t0YGA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate and upload CSV file to Google Cloud Storage (do it once)"
      ],
      "metadata": {
        "id": "U56LWFoC21_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(NO_PATH_BUCKET_NAME)\n",
        "if not os.path.isdir(f\"mnist_{TIMESTAMP}\"):\n",
        "    os.mkdir(f\"mnist_{TIMESTAMP}\")\n",
        "for i, x in enumerate(X_data):\n",
        "    # Console print\n",
        "    if i % 1000 == 0:\n",
        "        print('Uploading image {image}'.format(image=i))\n",
        "    # Reshape and export image\n",
        "    img = array_to_img(x=x.reshape(28, 28, 1))\n",
        "    img.save(fp=f\"mnist_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "    # Add info to data frame\n",
        "    files.iloc[i, 1] = f\"{BUCKET_NAME}/mnist_{TIMESTAMP}/image_{str(i)}.jpg\"\n",
        "    files.iloc[i, 2] = y_data[i]\n",
        "    # Upload to GCP\n",
        "    blob = bucket.blob(f\"mnist_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "    blob.upload_from_filename(f\"mnist_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "    # Delete image file\n",
        "    os.remove(f\"mnist_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "# Export CSV file\n",
        "files.to_csv(path_or_buf='mnist_map.csv', header=False, index=False)\n",
        "blob = bucket.blob(f\"mnist_{TIMESTAMP}/mnist_map.csv\")\n",
        "blob.upload_from_filename(f\"mnist_{TIMESTAMP}/mnist_map.csv\")"
      ],
      "metadata": {
        "id": "ccwHAKUPbkD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save generated CSV also to Google Drive, in order to have a backup"
      ],
      "metadata": {
        "id": "duwMklITqECf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/\"):\n",
        "    os.mkdir(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/\")\n",
        "files.to_csv(path_or_buf=f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/mnist_map.csv\", header=False, index=False)"
      ],
      "metadata": {
        "id": "i8XgB0CQptQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FETNifeALpVU"
      },
      "source": [
        "Set IMPORT_FILE variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-VKRU6DLre1"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = f\"gs://{NO_PATH_BUCKET_NAME}/mnist_{TIMESTAMP}/mnist_map.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new dataset from CSV file"
      ],
      "metadata": {
        "id": "7j9wvEwDE5eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_mnist_dataset = aiplatform.ImageDataset.create(\n",
        "    display_name=f\"mnist_dataset_{TIMESTAMP}\",\n",
        "    gcs_source=[IMPORT_FILE],\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7-y_cZWOj0w",
        "outputId": "e9d1b40d-9265-44e9-94b8-c27a6b42f567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ImageDataset\n",
            "Create ImageDataset backing LRO: projects/152908619293/locations/europe-west4/datasets/2840350795847696384/operations/1602647049134669824\n",
            "ImageDataset created. Resource name: projects/152908619293/locations/europe-west4/datasets/2840350795847696384\n",
            "To use this ImageDataset in another session:\n",
            "ds = aiplatform.ImageDataset('projects/152908619293/locations/europe-west4/datasets/2840350795847696384')\n",
            "Importing ImageDataset data: projects/152908619293/locations/europe-west4/datasets/2840350795847696384\n",
            "Import ImageDataset data backing LRO: projects/152908619293/locations/europe-west4/datasets/2840350795847696384/operations/8772377655908499456\n",
            "ImageDataset data imported. Resource name: projects/152908619293/locations/europe-west4/datasets/2840350795847696384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a training pipeline"
      ],
      "metadata": {
        "id": "Vf1tzZtCPUNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_job = aiplatform.AutoMLImageTrainingJob(\n",
        "    display_name=\"mnist_job_\" + TIMESTAMP,\n",
        "    prediction_type=\"classification\",\n",
        "    multi_label=False,\n",
        "    model_type=\"CLOUD\",\n",
        "    base_model=None,\n",
        ")\n",
        "\n",
        "print(train_job)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1HZf4dGPWvC",
        "outputId": "d235fd03-98cf-4106-8495-727e907dec81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<google.cloud.aiplatform.training_jobs.AutoMLImageTrainingJob object at 0x7f4608805710>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the training pipeline to train the model"
      ],
      "metadata": {
        "id": "aI_weq6LPpG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_job.run(\n",
        "    dataset=train_mnist_dataset,\n",
        "    model_display_name=\"mnist_model_\" + TIMESTAMP,\n",
        "    budget_milli_node_hours=8000,\n",
        "    disable_early_stopping=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jmdB2ktPogy",
        "outputId": "ec80fbf7-dd7d-4d65-efde-514f1c684552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No dataset split provided. The service will use a default split.\n",
            "View Training:\n",
            "https://console.cloud.google.com/ai/platform/locations/europe-west4/training/2419275225804832768?project=152908619293\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_PENDING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_PENDING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "AutoMLImageTrainingJob run completed. Resource name: projects/152908619293/locations/europe-west4/trainingPipelines/2419275225804832768\n",
            "Model available at projects/152908619293/locations/europe-west4/models/8470901463176970240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Only if already done:** retrieve the model by ID"
      ],
      "metadata": {
        "id": "vDyKtmu8PxY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = aiplatform.Model(\"projects/152908619293/locations/europe-west4/models/8470901463176970240@1\")"
      ],
      "metadata": {
        "id": "0Qm89-__Qihi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch prediction test"
      ],
      "metadata": {
        "id": "o-pu4uQz6vHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get dataframe with GCS path of all images of test set"
      ],
      "metadata": {
        "id": "94LsM5uq2lcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_df = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/mnist_map.csv\", header=None)\n",
        "batch_df.columns = [\"set\", \"path\", \"label\"]\n",
        "batch_df = batch_df.loc[batch_df['set']==\"TEST\"]\n",
        "batch_df.reset_index(drop=True, inplace=True)\n",
        "batch_df.drop([\"set\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "iBBS3JX32jPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload to GCS the JSONL file of batch prediction"
      ],
      "metadata": {
        "id": "Djxq5h3H7t7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "gcs_input_uri = f\"{BUCKET_NAME}/mnist_{TIMESTAMP}/batch_test.jsonl\"\n",
        "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
        "    for index, row in batch_df.iterrows():\n",
        "        data = {\"content\": row[\"path\"], \"mime_type\": \"image/jpeg\"}\n",
        "        f.write(json.dumps(data) + \"\\n\")\n",
        "\n",
        "print(gcs_input_uri)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYJyVAfy6RVe",
        "outputId": "c454a9cb-b9b8-4f37-cd2e-28e3c5719f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://bucket-mnist-1/mnist_20220530154112/batch_test.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and run batch prediction job"
      ],
      "metadata": {
        "id": "YElqAXUL70Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_predict_job = model.batch_predict(\n",
        "    job_display_name=\"mnist_batch_prediction_job_\" + TIMESTAMP,\n",
        "    gcs_source=gcs_input_uri,\n",
        "    gcs_destination_prefix=f\"{BUCKET_NAME}/mnist_{TIMESTAMP}/predictions/\",\n",
        "    sync=True,\n",
        ")\n",
        "\n",
        "print(batch_predict_job)"
      ],
      "metadata": {
        "id": "affIg0sN74Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve batch prediction results"
      ],
      "metadata": {
        "id": "xdMoVa-58dCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/batch_pred_test.csv\"):\n",
        "    bp_iter_outputs = batch_predict_job.iter_outputs()\n",
        "\n",
        "    prediction_results = list()\n",
        "    for blob in bp_iter_outputs:\n",
        "        if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
        "            prediction_results.append(blob.name)\n",
        "\n",
        "    pred_df = pd.DataFrame()\n",
        "    for prediction_result in prediction_results:\n",
        "        gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
        "        with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
        "            for line in gfile.readlines():\n",
        "                line = json.loads(line)\n",
        "                new_line = {\"path\":line.get(\"instance\").get(\"content\"), \"predicted_value\":line.get(\"prediction\").get(\"displayNames\")[0]}\n",
        "                pred_df = pred_df.append(new_line, ignore_index=True)\n",
        "\n",
        "    pred_df.sort_values(\"path\", inplace=True)\n",
        "    pred_df.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/batch_pred_test.csv\", index=False)\n",
        "else: \n",
        "    pred_df = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/batch_pred_test.csv\")"
      ],
      "metadata": {
        "id": "fe2BcT0C8csQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check accuracy of the model"
      ],
      "metadata": {
        "id": "7CWzQQZjKxYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "actual_y_test = batch_df[\"label\"].astype(int).to_list()\n",
        "predicted_y_test = pred_df[\"predicted_value\"].astype(int).to_list()\n",
        "print(confusion_matrix(actual_y_test,predicted_y_test))\n",
        "print(f\"Accuracy of the CNN: {accuracy_score(actual_y_test,predicted_y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU1h5wSyKw_9",
        "outputId": "7c849b2f-db40-492b-e28c-cdb69e9b0875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 975    0    0    0    0    2    1    0    0    2]\n",
            " [   0 1129    1    0    0    0    2    3    0    0]\n",
            " [   0    1 1018    5    0    5    3    0    0    0]\n",
            " [   0    0    1  947    0   59    0    1    2    0]\n",
            " [   0    0    0    0  973    0    1    0    0    8]\n",
            " [   1    0    0   10    0  878    2    1    0    0]\n",
            " [   1    1    1    0    0    6  947    0    2    0]\n",
            " [   0    2    4    0    2    1    0 1018    0    1]\n",
            " [   0    0    3    1    0    2    0    0  967    1]\n",
            " [   0    0    0    1    5    2    1    2    3  995]]\n",
            "Accuracy of the CNN: 0.9847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robustness"
      ],
      "metadata": {
        "id": "jq4HgCnUSPRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the robustness with RobustBench (CUDA required)\n"
      ],
      "metadata": {
        "id": "Cs_UNb5GU42z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.reshape(10000,28,28,1)\n",
        "X_test_tensor = torch.from_numpy(X_test).to(torch.float32)\n",
        "y_test_tensor = torch.from_numpy(y_test).to(torch.float32)"
      ],
      "metadata": {
        "id": "brBETyMeTo0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autoattack import AutoAttack\n",
        "adversary = AutoAttack(model, norm='Linf', eps=8/255, version='custom', attacks_to_run=['apgd-ce', 'apgd-dlr'])\n",
        "adversary.apgd.n_restarts = 1\n",
        "x_adv = adversary.run_standard_evaluation(X_test_tensor, y_test_tensor)"
      ],
      "metadata": {
        "id": "ACdDihyOS0Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Privacy"
      ],
      "metadata": {
        "id": "_5IAfOUXV07H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy and test model"
      ],
      "metadata": {
        "id": "beU1M6JvPqKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = aiplatform.Endpoint.create(\n",
        "    display_name=f\"mnist_endpoint_{TIMESTAMP}\",\n",
        "    project=PROJECT,\n",
        "    location=REGION,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKhbPvRlwVmp",
        "outputId": "7131bd0b-7e17-41c9-9ce4-52f1535f17f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Endpoint\n",
            "Create Endpoint backing LRO: projects/152908619293/locations/europe-west4/endpoints/1281960189242638336/operations/3353773247995838464\n",
            "Endpoint created. Resource name: projects/152908619293/locations/europe-west4/endpoints/1281960189242638336\n",
            "To use this Endpoint in another session:\n",
            "endpoint = aiplatform.Endpoint('projects/152908619293/locations/europe-west4/endpoints/1281960189242638336')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually set endpoint ID, taken from the returned value"
      ],
      "metadata": {
        "id": "F7injIzDPvOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENDPOINT_ID = \"1281960189242638336\"\n",
        "endpoint = aiplatform.Endpoint(ENDPOINT_ID)"
      ],
      "metadata": {
        "id": "9QMhVfrKxE48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.deploy(endpoint=endpoint,\n",
        "             machine_type=\"n1-standard-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpgWyDnmwRNx",
        "outputId": "ca71d178-e341-43bc-f829-ec6fe267e809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploying model to Endpoint : projects/152908619293/locations/europe-west4/endpoints/1281960189242638336\n",
            "Model does not support dedicated deployment resources. The machine_type, accelerator_type and accelerator_count,autoscaling_target_accelerator_duty_cycle,autoscaling_target_cpu_utilization parameters are ignored.\n",
            "Deploy Endpoint model backing LRO: projects/152908619293/locations/europe-west4/endpoints/1281960189242638336/operations/7281756547993042944\n",
            "Endpoint model deployed. Resource name: projects/152908619293/locations/europe-west4/endpoints/1281960189242638336\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.cloud.aiplatform.models.Endpoint object at 0x7fa229b43910> \n",
              "resource name: projects/152908619293/locations/europe-west4/endpoints/1281960189242638336"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import base64\n",
        "from google.cloud.aiplatform.gapic.schema import predict\n",
        "\n",
        "def predict_classification_online(gcs_input_uri):\n",
        "    endpoint = aiplatform.Endpoint(ENDPOINT_ID)\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(NO_PATH_BUCKET_NAME)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    with tf.io.gfile.GFile(gcs_input_uri, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            line = json.loads(line)\n",
        "            blob = bucket.blob(line.get(\"content\").replace(\"gs://bucket-mnist-1/\",\"\"))\n",
        "            image_to_encode = blob.download_as_bytes()\n",
        "            encoded_image = base64.b64encode(image_to_encode).decode(\"utf-8\")\n",
        "            instance = predict.instance.ImageClassificationPredictionInstance(\n",
        "                    content=encoded_image,\n",
        "                    mime_type=line.get(\"mime/type\")\n",
        "            ).to_value()\n",
        "            instances = [instance]\n",
        "\n",
        "            response = endpoint.predict(instances=instances)\n",
        "\n",
        "            for prediction in response.predictions:\n",
        "                id = 0\n",
        "                tmp = -999\n",
        "                for i, value in enumerate(prediction.get(\"confidences\")):\n",
        "                    if value>tmp:\n",
        "                        tmp = value\n",
        "                        id = i\n",
        "                results.append(prediction.get(\"displayNames\")[id])\n",
        "        \n",
        "    return results"
      ],
      "metadata": {
        "id": "fziJetO7xQsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.reshape(10000,28,28,1)\n",
        "size_noisy_data = 100"
      ],
      "metadata": {
        "id": "ok_7mGH7_I5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define transformation and transform data"
      ],
      "metadata": {
        "id": "mcqO5eAuAK4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "metadata": {
        "id": "MvTHboi0AKXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try with one value of std dev (intensity) of noise"
      ],
      "metadata": {
        "id": "ntoKtSM3M0Pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import transforms\n",
        "gaussian_noise=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(0., 0.1)\n",
        "])\n",
        "\n",
        "X_test_trans = []\n",
        "for img in X_test[:size_noisy_data]:\n",
        "    img = gaussian_noise(img)\n",
        "    X_test_trans.append(img)\n",
        "X_test_trans = torch.cat(X_test_trans)"
      ],
      "metadata": {
        "id": "DVOzK3xFn1CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save transformed data to GCS"
      ],
      "metadata": {
        "id": "HhSDCKzoDWnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a single file for all the images"
      ],
      "metadata": {
        "id": "h4aCBHXdiEQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset placeholder\n",
        "files = pd.DataFrame({'part': np.repeat('noisy', size_noisy_data),\n",
        "                      'file': np.repeat('file', size_noisy_data),\n",
        "                      'label': np.repeat('label', size_noisy_data)})"
      ],
      "metadata": {
        "id": "VrmEFW0uiEQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_trans = X_test_trans.cpu().numpy().reshape(size_noisy_data,28,28,1)"
      ],
      "metadata": {
        "id": "qcln5SPojHrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate and upload CSV file to Google Cloud Storage (do it once)"
      ],
      "metadata": {
        "id": "rsCKES04iERA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(NO_PATH_BUCKET_NAME)\n",
        "if not os.path.isdir(f\"mnist_noisy_{TIMESTAMP}\"):\n",
        "    os.mkdir(f\"mnist_noisy_{TIMESTAMP}\")\n",
        "for i, x in enumerate(X_test_trans):\n",
        "    # Reshape and export image\n",
        "    img = array_to_img(x=x.reshape(28, 28, 1))\n",
        "    img.save(fp=f\"mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "    # Add info to data frame\n",
        "    files.iloc[i, 1] = f\"{BUCKET_NAME}/mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\"\n",
        "    files.iloc[i, 2] = y_test[i]\n",
        "    # Upload to GCP\n",
        "    blob = bucket.blob(f\"mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "    blob.upload_from_filename(f\"mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "    # Delete image file\n",
        "    os.remove(f\"mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "# Export CSV file\n",
        "files.to_csv(path_or_buf=f'mnist_noisy_{TIMESTAMP}/mnist_noisy_map.csv', header=False, index=False)\n",
        "blob = bucket.blob(f\"mnist_noisy_{TIMESTAMP}/mnist_noisy_map.csv\")\n",
        "blob.upload_from_filename(f\"mnist_noisy_{TIMESTAMP}/mnist_noisy_map.csv\")"
      ],
      "metadata": {
        "id": "KO9jwr6jiERA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save generated CSV to Google Drive"
      ],
      "metadata": {
        "id": "vgcvdvf1iERB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/\"):\n",
        "    os.mkdir(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/\")\n",
        "files.to_csv(path_or_buf=f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/mnist_noisy_map.csv\", header=False, index=False)"
      ],
      "metadata": {
        "id": "1VltxbnEiERC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get dataframe with GCS path of all images of test set"
      ],
      "metadata": {
        "id": "8Qtre0xgmFYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_df = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/mnist_noisy_map.csv\", header=None)\n",
        "batch_df.columns = [\"set\", \"path\", \"label\"]\n",
        "batch_df.drop([\"set\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "lB_8kkSvmFYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload to GCS the JSONL file of batch prediction"
      ],
      "metadata": {
        "id": "6Ex-WZrYmFYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "gcs_input_uri = f\"{BUCKET_NAME}/mnist_noisy_{TIMESTAMP}/batch_test.jsonl\"\n",
        "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
        "    for index, row in batch_df.iterrows():\n",
        "        data = {\"content\": row[\"path\"], \"mime_type\": \"image/jpeg\"}\n",
        "        f.write(json.dumps(data) + \"\\n\")\n",
        "\n",
        "print(gcs_input_uri)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e72e98b-9a08-4ec1-8d4e-6f46538fa50a",
        "id": "0Wp7TGfmmFYg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://bucket-mnist-1/mnist_noisy_20220530154112/batch_test.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pred_results = predict_classification_online(gcs_input_uri)\n",
        "pred_results = list(map(int, pred_results))\n",
        "actual_values = batch_df[\"label\"].to_list()\n",
        "print(confusion_matrix(actual_values,pred_results))\n",
        "print(f\"Accuracy of the CNN: {accuracy_score(actual_values,pred_results)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fZGZ8ZLN3PW",
        "outputId": "1a01574f-acb4-4695-f92f-12c04070594b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  4  1  0  0  0  0  9  0  0]\n",
            " [ 0  0  4  0  0  0  0  1  3  0]\n",
            " [ 0  0  0  5  0  0  0  1  5  0]\n",
            " [ 0  0  1  0  9  0  0  1  3  0]\n",
            " [ 0  0  0  3  0  1  0  0  3  0]\n",
            " [ 2  0  0  0  0  0  1  0  7  0]\n",
            " [ 0  0  2  1  0  0  0 12  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  2  0]\n",
            " [ 0  0  0  0  0  0  0  0  2  9]]\n",
            "Accuracy of the CNN: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop all the things to analyze the behavior at the variation of the intensity of the noise"
      ],
      "metadata": {
        "id": "iGZHNNO0RJKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "from torchvision.transforms import transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "size_noisy_data = 100\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(NO_PATH_BUCKET_NAME)\n",
        "for intensity in np.linspace(0., 0.6, 14):\n",
        "    gaussian_noise=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        AddGaussianNoise(0., intensity)\n",
        "    ])\n",
        "\n",
        "    X_test_trans = []\n",
        "    for img in X_test[:size_noisy_data]:\n",
        "        img = gaussian_noise(img)\n",
        "        X_test_trans.append(img)\n",
        "    X_test_trans = torch.cat(X_test_trans)\n",
        "\n",
        "\n",
        "    # Dataset placeholder\n",
        "    files = pd.DataFrame({'part': np.repeat('noisy', size_noisy_data),\n",
        "                        'file': np.repeat('file', size_noisy_data),\n",
        "                        'label': np.repeat('label', size_noisy_data)})\n",
        "    \n",
        "    X_test_trans = X_test_trans.cpu().numpy().reshape(size_noisy_data,28,28,1)\n",
        "    \n",
        "    if not os.path.isdir(f\"mnist_noisy_{TIMESTAMP}\"):\n",
        "        os.mkdir(f\"mnist_noisy_{TIMESTAMP}\")\n",
        "    for i, x in enumerate(X_test_trans):\n",
        "        # Reshape and export image\n",
        "        img = array_to_img(x=x.reshape(28, 28, 1))\n",
        "        img.save(fp=f\"mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "        # Add info to data frame\n",
        "        files.iloc[i, 1] = f\"{BUCKET_NAME}/mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\"\n",
        "        files.iloc[i, 2] = y_test[i]\n",
        "        # Upload to GCP\n",
        "        blob = bucket.blob(f\"mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "        blob.upload_from_filename(f\"mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "        # Delete image file\n",
        "        os.remove(f\"mnist_noisy_{TIMESTAMP}/image_{str(i)}.jpg\")\n",
        "    # Export CSV file\n",
        "    files.to_csv(path_or_buf=f'mnist_noisy_{TIMESTAMP}/mnist_noisy_map.csv', header=False, index=False)\n",
        "    blob = bucket.blob(f\"mnist_noisy_{TIMESTAMP}/mnist_noisy_map.csv\")\n",
        "    blob.upload_from_filename(f\"mnist_noisy_{TIMESTAMP}/mnist_noisy_map.csv\")\n",
        "\n",
        "    if not os.path.isdir(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/\"):\n",
        "        os.mkdir(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/\")\n",
        "    files.to_csv(path_or_buf=f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/mnist_noisy_map.csv\", header=False, index=False)\n",
        "\n",
        "    batch_df = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/mnist_noisy_map.csv\", header=None)\n",
        "    batch_df.columns = [\"set\", \"path\", \"label\"]\n",
        "    batch_df.drop([\"set\"], axis=1, inplace=True)\n",
        "\n",
        "    gcs_input_uri = f\"{BUCKET_NAME}/mnist_noisy_{TIMESTAMP}/batch_test.jsonl\"\n",
        "    with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
        "        for index, row in batch_df.iterrows():\n",
        "            data = {\"content\": row[\"path\"], \"mime_type\": \"image/jpeg\"}\n",
        "            f.write(json.dumps(data) + \"\\n\")\n",
        "\n",
        "    pred_results = predict_classification_online(gcs_input_uri)\n",
        "    pred_results = list(map(int, pred_results))\n",
        "    actual_values = batch_df[\"label\"].to_list()\n",
        "    print(f\"Accuracy of the CNN with standard dev of noise {intensity.round(3)}: {accuracy_score(actual_values,pred_results)}\")"
      ],
      "metadata": {
        "id": "2ky27h_6RImA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa8987b-c1d7-401c-f03e-7d87c8d164eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the CNN with standard dev of noise 0.0: 1.0\n",
            "Accuracy of the CNN with standard dev of noise 0.046: 0.84\n",
            "Accuracy of the CNN with standard dev of noise 0.092: 0.63\n",
            "Accuracy of the CNN with standard dev of noise 0.138: 0.37\n",
            "Accuracy of the CNN with standard dev of noise 0.185: 0.3\n",
            "Accuracy of the CNN with standard dev of noise 0.231: 0.16\n",
            "Accuracy of the CNN with standard dev of noise 0.277: 0.13\n",
            "Accuracy of the CNN with standard dev of noise 0.323: 0.1\n",
            "Accuracy of the CNN with standard dev of noise 0.369: 0.1\n",
            "Accuracy of the CNN with standard dev of noise 0.415: 0.08\n",
            "Accuracy of the CNN with standard dev of noise 0.462: 0.13\n",
            "Accuracy of the CNN with standard dev of noise 0.508: 0.11\n",
            "Accuracy of the CNN with standard dev of noise 0.554: 0.08\n",
            "Accuracy of the CNN with standard dev of noise 0.6: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and run batch prediction job"
      ],
      "metadata": {
        "id": "ashTPm0dmFYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_predict_job = model.batch_predict(\n",
        "    job_display_name=\"mnist_noisy_batch_prediction_job_\" + TIMESTAMP,\n",
        "    gcs_source=gcs_input_uri,\n",
        "    gcs_destination_prefix=f\"{BUCKET_NAME}/mnist_noisy_{TIMESTAMP}/predictions/\",\n",
        "    sync=True,\n",
        ")\n",
        "\n",
        "print(batch_predict_job)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3vwW_s-mFYi",
        "outputId": "e645a9c7-ad96-4491-dfc6-04e5cd53102e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating BatchPredictionJob\n",
            "<google.cloud.aiplatform.jobs.BatchPredictionJob object at 0x7f883940c5d0> is waiting for upstream dependencies to complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve batch prediction results"
      ],
      "metadata": {
        "id": "etGQF1_fmFYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/noisy_batch_pred_test.csv\"):\n",
        "    bp_iter_outputs = batch_predict_job.iter_outputs()\n",
        "\n",
        "    prediction_results = list()\n",
        "    for blob in bp_iter_outputs:\n",
        "        if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
        "            prediction_results.append(blob.name)\n",
        "\n",
        "    pred_df = pd.DataFrame()\n",
        "    for prediction_result in prediction_results:\n",
        "        gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
        "        with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
        "            for line in gfile.readlines():\n",
        "                line = json.loads(line)\n",
        "                new_line = {\"path\":line.get(\"instance\").get(\"content\"), \"predicted_value\":line.get(\"prediction\").get(\"displayNames\")[0]}\n",
        "                pred_df = pred_df.append(new_line, ignore_index=True)\n",
        "\n",
        "    pred_df.sort_values(\"path\", inplace=True)\n",
        "    pred_df.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/noisy_batch_pred_test.csv\", index=False)\n",
        "else: \n",
        "    pred_df = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/MNIST/{TIMESTAMP}/noisy_batch_pred_test.csv\")"
      ],
      "metadata": {
        "id": "Ect3ICYPmFYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check accuracy of the model"
      ],
      "metadata": {
        "id": "YHqFyPeAmFYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "actual_y_test = batch_df[\"label\"].astype(int).to_list()\n",
        "predicted_y_test = pred_df[\"predicted_value\"].astype(int).to_list()\n",
        "print(confusion_matrix(actual_y_test,predicted_y_test))\n",
        "print(f\"Accuracy of the CNN: {accuracy_score(actual_y_test,predicted_y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced62848-3e65-4d9d-c1a8-adccc7e0b187",
        "id": "sx8SGmo5mFYl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 1 0]\n",
            " [0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2]]\n",
            "Accuracy of the CNN: 0.8\n"
          ]
        }
      ]
    }
  ]
}